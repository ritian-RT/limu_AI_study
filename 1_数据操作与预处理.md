# 数据操作
## 一.  
1.访问元素  

![1_1](image\1_1.png)

子区域中的1:3表示从第一行到第三行的区域；  
::3表示每隔三行取一次  

2.首先，我们导入torch。请注意，虽然它被称为PyTorch，但是代码中使用torch而不是pytorch。   
``import torch``

3.我们可以使用 arange 创建一个行向量 x   
``x = torch.arange(12)``  
``tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])``

4.可以通过张量的shape属性来访问张量（沿每个轴的长度）的形状  
``x.shape``  
``torch.Size([12])``

5.如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）  
``x.numel()``  

6.要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数  
``X = x.reshape(3, 4)``  
``tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])``

7.我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。  
``torch.zeros((2, 3, 4))``  
若都设置为1  
``torch.ones((2, 3, 4))``  

8.当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。  
``torch.randn(3, 4)``  
``tensor([[-0.0135,  0.0665,  0.0912,  0.3212],
        [ 1.4653,  0.1843, -1.6995, -0.3036],
        [ 1.7646,  1.0450,  0.2457, -0.7732]])``

 ## 二
 1.简单运算符  
 ``x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2]) ``  
x + y, x - y, x * y, x / y, x ** y  
**运算符是求幂运算

output:  
``(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))``

 2.我们也可以把多个张量连结（concatenate）在一起  
 ``X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])``  
``torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)``  
dim=0可以理解为按行进行堆叠，dim=1可以理解为按列进行一对一的拼接  
output:  
>(tensor([[ 0.,  1.,  2.,  3.],  
         [ 4.,  5.,  6.,  7.],  
         [ 8.,  9., 10., 11.],  
         [ 2.,  1.,  4.,  3.],  
         [ 1.,  2.,  3.,  4.],  
         [ 4.,  3.,  2.,  1.]])  
 ``tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))``  

3.通过逻辑运算符构建二元张量  
``X == Y``  
``tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])``

4.求和  
``X.sum()``  

## 三.广播机制  
即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作  
1.通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；  
2.对生成的数组执行按元素操作。  
``a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))``  
由于a和b分别是 3×1 和 1×2 矩阵，如果让它们相加，它们的形状不匹配。   
我们将两个矩阵广播为一个更大的 3×2 矩阵，如下所示：**矩阵a将复制列， 矩阵b将复制行**，然后再按元素相加。  
output： ``tensor([[0, 1],
        [1, 2],
        [2, 3]])``

## 四.索引和切片  
就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。  

## 五.节省内存  
运行一些操作可能会导致为新结果分配内存  
1.例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。  
在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。   
运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。  
>before = id(Y)  
Y = Y + X  
id(Y) == before  

output:False  
这可能是不可取的，原因有两个：
>首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；  
如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

2.(执行原地操作)非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组  
例如Y[:] = <expression>。 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全 0 的块。
>Z = torch.zeros_like(Y)  
print('id(Z):', id(Z))  
Z[:] = X + Y  
print('id(Z):', id(Z))  

output:
>id(Z): 140327634811696  
id(Z): 140327634811696


如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。  
>before = id(X)  
X += Y  
id(X) == before  

# 数据预处理  
## 一.读取数据集
1.我们首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv中。   
以其他格式存储的数据也可以通过类似的方式进行处理。 下面我们将数据集按行写入CSV文件中。  
>import os  
os.makedirs(os.path.join('..', 'data'), exist_ok=True)  
data_file = os.path.join('..', 'data', 'house_tiny.csv')  
with open(data_file, 'w') as f:  
    f.write('NumRooms,Alley,Price\n')  # 列名  
    f.write('NA,Pave,127500\n')  # 每行表示一个数据样本  
    f.write('2,NA,106000\n')  
    f.write('4,NA,178100\n')  
    f.write('NA,NA,140000\n')  

2.要从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。  
>import pandas as pd  
data = pd.read_csv(data_file)  
print(data)  

## 二.处理缺失值  
``
data = {
    'A': [1, 2, np.nan, 4],
    'B': [np.nan, 1, 2, 3],
    'C': [5, 6, 7, 8]
}``    

``inputs = pd.DataFrame(data)
inputs = inputs.fillna(inputs.mean())
print(inputs)``

inputs：表示一个包含数据的 Pandas DataFrame 或 Series。  
inputs.mean()：计算 inputs 中每一列的平均值。  
fillna()：用指定的值替换缺失值（NaN）。  
output:
*   A    B  C
* 0  1.0  2.0  5
* 1  2.0  1.0  6
* 2  2.333333  2.0  7
* 3  4.0  3.0  8

## 三.转换为张量格式  
现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式。  

``import torch
X = torch.tensor(inputs.to_numpy(dtype=float))  
y = torch.tensor(outputs.to_numpy(dtype=float))``

## 注意
1.不要随意更改变量  
例如：当b改变时，a也随之改变
>import torch  
a = torch.arange(12)  
b = a.reshape(3,4)  
b[:] = 1  
print(a)   
print(b)  

output:
>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])  
tensor([[1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1]])



## 作业：  
1.删除缺失值最多的列。  
2.将预处理后的数据集转换为张量格式。  

1.
>missing_count = data.isnull().sum()  
max_column = missing_count.idxmax()  
data = data.drop(columns=[max_column])  
print(data)

解释：  
isnull()：返回一个布尔 DataFrame，表示每个元素是否为缺失值（NaN）。  
sum()：对布尔值求和，计算每列的缺失值数量。  
idxmax()：返回缺失值数量最多的列的名称。  
drop()：删除指定的列，columns 参数指定要删除的列名称列表。  

2.
>import torch  
data_clean  = data.to_numpy()  
tensor = torch.tensor(data_clean, dtype=torch.float32)  
print(tensor)  

转换为 NumPy 数组：使用 to_numpy() 方法将清理后的 DataFrame 转换为 NumPy 数组。  
转换为 PyTorch 张量：使用 torch.tensor() 将 NumPy 数组转换为 PyTorch 张量，并指定数据类型为 float32。